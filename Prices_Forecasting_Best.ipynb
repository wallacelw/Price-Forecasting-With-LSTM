{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wallacelw/Price-Forecasting-With-LSTM/blob/main/Prices_Forecasting_Best.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok-lTqm4kgyR"
      },
      "source": [
        "Datasets available @\n",
        "\n",
        "BTC, AAPL, MSFT, TSLA, ^IXIC(NASDAQ), ^BVSP(IBOVESPA):\n",
        "https://finance.yahoo.com/\n",
        "\n",
        "S&P 500:\n",
        "https://www.kaggle.com/datasets/andrewmvd/sp-500-stocks?select=sp500_index.csv\n",
        "\n",
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "cs_6VBglkjxk",
        "outputId": "6fc38521-90a5-4acb-d9ae-65b61320188c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5a3dfaa3e62a>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSFT.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5a3dfaa3e62a>\u001b[0m in \u001b[0;36mload_df\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprice_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MSFT.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def str_to_datetime(s):\n",
        "    \"\"\" Converts a string object to the respective datetime objects\"\"\"\n",
        "\n",
        "    year, month, day = [int(i) for i in s.split('-')]\n",
        "    return datetime.datetime(year=year, month=month, day=day)\n",
        "\n",
        "\n",
        "price_dict = {\n",
        "    \"Adj Close\" : \"Price\",\n",
        "    \"S&P500\" : \"Price\",\n",
        "}\n",
        "\n",
        "def load_df(filename):\n",
        "    \"\"\"\n",
        "    Create dataframe, filter only Price column,\n",
        "    convert date to datetime and make it the index\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    df.rename(columns = price_dict, inplace = True)\n",
        "\n",
        "    # Univariate analysis\n",
        "    df = df[[\"Date\", \"Price\"]]\n",
        "\n",
        "    # Convert date type objects to datetime object\n",
        "    df[\"Date\"] = df[\"Date\"].apply(str_to_datetime)\n",
        "\n",
        "    # Turn \"Date\" Column into dataframe index\n",
        "    df.index = df.pop(\"Date\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df = load_df(\"MSFT.csv\")\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDoNnLl8xKgH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(df.index, df[\"Price\"])\n",
        "plt.title(\"Full Dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnu4eoP7xgX7"
      },
      "outputs": [],
      "source": [
        "# Choose the amount of days to consider from the dataset\n",
        "days = 5000 # ~13 years\n",
        "\n",
        "# numbers of days to consider in the input of the model\n",
        "lookback = 15 #\n",
        "\n",
        "\n",
        "def df_to_windowed(fullDF, n=lookback, daysSelected=days):\n",
        "    \"\"\"\n",
        "    Create a windowed Dataframe (converting into a supervised problem).\n",
        "    Therefore, the last {lookback} days prices will be the (input)\n",
        "    and will generate the next day price (output)\n",
        "    \"\"\"\n",
        "\n",
        "    tmp_df = pd.DataFrame()\n",
        "    for i in range(n, 0, -1):\n",
        "        tmp_df[f\"Last-{i} Price\"] = fullDF[\"Price\"].shift(periods=i)\n",
        "    tmp_df[\"Price\"] = fullDF[\"Price\"]\n",
        "\n",
        "    return tmp_df.dropna()[-daysSelected:]\n",
        "\n",
        "\n",
        "windowed_df = df_to_windowed(df)\n",
        "\n",
        "windowed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dLI_87sP8kj"
      },
      "outputs": [],
      "source": [
        "windowed_df[\"Price\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqpoNzUq5Yz7"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRZ5V-d4P8kj"
      },
      "outputs": [],
      "source": [
        "# Command to disable GPU:\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CkaFw6n5Xgm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# model input: (last {lookback} days prices, 1 feature = \"price\")\n",
        "models = [\n",
        "    Sequential([ # CNN+LSTM+Dropout\n",
        "       layers.Input((lookback, 1)),\n",
        "       layers.Conv1D(128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
        "       layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n",
        "       layers.LSTM(128, return_sequences=True),\n",
        "       layers.Flatten(),\n",
        "       layers.Dropout(0.3),\n",
        "       layers.Dense(128),\n",
        "       layers.Dense(1)\n",
        "    ]),\n",
        "\n",
        "    Sequential([ # LSTM\n",
        "        layers.Input((lookback, 1)),\n",
        "        layers.LSTM(128, return_sequences=True),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128),\n",
        "        layers.Dense(1)\n",
        "    ]),\n",
        "\n",
        "    Sequential([ # CNN\n",
        "        layers.Input((lookback, 1)),\n",
        "        layers.Conv1D(128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
        "        layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128),\n",
        "        layers.Dense(1)\n",
        "    ]),\n",
        "\n",
        "    Sequential([ # Rede Neural Simples\n",
        "        layers.Input((lookback, 1)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128),\n",
        "        layers.Dense(128),\n",
        "        layers.Dense(1)\n",
        "    ]),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF4vQq8-P8kj"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "    print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwHUheOj6jSA"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "### Auxilary Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmpXMMRrH8bB"
      },
      "outputs": [],
      "source": [
        "# For each year, 60% train, 20% validation, 20% test\n",
        "def sliding_window_generator(windowed, trainSize=2100, valiSize=450, testSize=450, step=60):\n",
        "    \"\"\"\n",
        "    Sliding Window Generator\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(0, len(windowed) - trainSize - testSize - valiSize + 1, step):\n",
        "        train_slice = windowed[i : i+trainSize]\n",
        "        vali_slice = windowed[i+trainSize : i+trainSize+valiSize]\n",
        "        test_slice = windowed[i+trainSize+valiSize : i+trainSize+valiSize+testSize]\n",
        "        yield (train_slice, vali_slice, test_slice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPTW4TzlP8kk"
      },
      "outputs": [],
      "source": [
        "# Plot windows' intervals and count numbers of windows\n",
        "plot_generator = sliding_window_generator(windowed_df)\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "windows_cnt = 0\n",
        "for train, vali, test in plot_generator:\n",
        "    plt.axvline(train.index[0], color=\"tab:gray\")\n",
        "    plt.plot(train.index, train[\"Price\"], color=\"tab:blue\")\n",
        "    plt.plot(vali.index, vali[\"Price\"], color=\"tab:orange\")\n",
        "    plt.plot(test.index, test[\"Price\"], color=\"tab:green\")\n",
        "    plt.axvline(test.index[-1], color=\"tab:gray\")\n",
        "    windows_cnt += 1;\n",
        "\n",
        "plt.title(f\"Number of Selected Windows: {windows_cnt}\")\n",
        "plt.legend([\n",
        "    \"Training Observations\",\n",
        "    \"Validation Observations\",\n",
        "    \"Testing Observations\",\n",
        "])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYpV94ee--mq"
      },
      "outputs": [],
      "source": [
        "def split_xy(windowedNP):\n",
        "    \"\"\"\n",
        "    Split np.array into X and y\n",
        "    \"\"\"\n",
        "\n",
        "    X = windowedNP[:, :-1]\n",
        "    y = windowedNP[:, -1]\n",
        "    return (X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgrRyNNrP8kk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def compute_accuracy_and_cm(y_val, y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Computes the accuracy score and the confusion matrix\n",
        "    For simplicity, zero price change are considered as positive\n",
        "    \"\"\"\n",
        "\n",
        "    sz = len(y_test)\n",
        "    y_ref = np.append(y_val[-1], y_test)\n",
        "\n",
        "    y_test_label = np.zeros(sz)\n",
        "    y_pred_label = np.zeros(sz)\n",
        "\n",
        "    acc = 0\n",
        "    for i in range(sz):\n",
        "        y_test_label[i] = 1 if ((y_test[i] - y_ref[i]) >= 0) else -1\n",
        "        y_pred_label[i]  = 1 if ((y_pred[i] - y_ref[i]) >= 0) else -1\n",
        "\n",
        "        if y_test_label[i] == y_pred_label[i]:\n",
        "            acc += 1\n",
        "\n",
        "    cm = confusion_matrix(y_true=y_test_label, y_pred=y_pred_label)\n",
        "    return acc/sz, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLbkdFAAP8kk"
      },
      "outputs": [],
      "source": [
        "from matplotlib import patches\n",
        "patienceSelected = 50\n",
        "\n",
        "def plot_loss_curve(history, model_idx, i, patience=patienceSelected):\n",
        "    aux_list = [(val, i) for i, val in enumerate(history.history['combine_metric'])]\n",
        "    best = min(aux_list)\n",
        "    last = len(history.history['combine_metric'])\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(f\"Loss Curve for: Model {model_idx}, Window {i}\")\n",
        "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.plot(history.history[\"combine_metric\"], label=\"Combined Loss\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "\n",
        "    plt.axvline(last-1, color=\"tab:gray\", ymax=0.3, linestyle='--')\n",
        "    plt.axvline(last-patience-1, color=\"tab:gray\", ymax=0.3, linestyle='--')\n",
        "    plt.axvline(best[1], color=\"tab:red\", ymax=0.3, linestyle='--')\n",
        "\n",
        "    red_patch = patches.Patch(color=\"tab:red\", label=f\"best epoch={best[1]}\")\n",
        "    gray_patch = patches.Patch(color=\"tab:gray\", label=f\"Early Stop Limits ({last-patience-1}, {last-1})\")\n",
        "\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    handles.extend([red_patch, gray_patch])\n",
        "\n",
        "    plt.legend(handles=handles, loc=\"upper right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aupIon4mP8kk"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(dates, ys, metrics, model_idx, i):\n",
        "    dates_train, dates_vali, dates_test = dates\n",
        "    y_train, y_vali, y_test, y_result = ys\n",
        "    rmse, mae, mape, r2, acc = metrics\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(dates_train, y_train)\n",
        "    plt.plot(dates_vali, y_vali)\n",
        "    plt.plot(dates_test, y_test)\n",
        "    plt.plot(dates_test, y_result)\n",
        "    plt.legend([\n",
        "        \"Training Observations\",\n",
        "        \"Validation Observations\",\n",
        "        \"Testing Observations\",\n",
        "        \"Testing Predictions\"\n",
        "    ])\n",
        "    plt.title(f\"Model {model_idx}, Window {i}, RMSE={rmse:.3f}, MAE={mae:.3f}, MAPE={mape:.3f}, R2={r2:.3f}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiGiz0KKP8kk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusion_matrix(cm, metrics, model_idx, i):\n",
        "    rmse, mae, mape, r2, acc = metrics\n",
        "\n",
        "    cm_plt = ConfusionMatrixDisplay(cm, display_labels=[\"Positive\", \"Negative\"])\n",
        "    cm_plt.plot()\n",
        "    cm_plt.ax_.set(\n",
        "        title= f\"Model {model_idx}, Window {i}, Accuracy={acc:.3f}\",\n",
        "        xlabel= \"Predicted Price Change\",\n",
        "        ylabel= \"Actual Price Change\"\n",
        "    )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDkUeOgpP8kk"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping , Callback, ModelCheckpoint\n",
        "import h5py\n",
        "\n",
        "class CombineCallback(Callback):\n",
        "    def __init__(self, **kargs):\n",
        "        super(CombineCallback, self).__init__(**kargs)\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        val_factor = 0.2 # 80% training loss, 20% validation loss\n",
        "        logs['combine_metric'] = val_factor * logs['val_loss'] + (1-val_factor) * logs['loss']\n",
        "\n",
        "combined_cb = CombineCallback()\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=\"tmp_best_model.h5\",\n",
        "    monitor=\"combine_metric\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=False\n",
        ")\n",
        "earlyStop = EarlyStopping(monitor=\"combine_metric\", min_delta=0, patience=patienceSelected, mode=\"min\", verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIacTVqiP8kk"
      },
      "source": [
        "### Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYYuIVTuK1Mc"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "\n",
        "def cross_validation(model, generator, model_idx, flag_plot=0):\n",
        "    \"\"\"\n",
        "    Performs Cross validation for all models and all sliding windows;\n",
        "    Calculates the cross validation score (\"RMSE\", \"MAE\", \"MAPE\", \"R2\", \"Accuracy\");\n",
        "    Accuracy is computed by considering if the relative price change for day i was positive or negative\n",
        "    \"\"\"\n",
        "\n",
        "    cv_score = pd.DataFrame(columns=[\"RMSE\", \"MAE\", \"MAPE\", \"R2\", \"Acc\"])\n",
        "\n",
        "    for i, (train, vali, test) in enumerate(generator):\n",
        "\n",
        "        # Get Dates = [dates_train, dates_vali, dates_test]\n",
        "        dates = [i.index for i in [train, vali, test]]\n",
        "\n",
        "        # Get Scaled Data\n",
        "        scaler = StandardScaler()\n",
        "        X_train_sc, y_train_sc = split_xy(scaler.fit_transform(train))\n",
        "        X_vali_sc, y_vali_sc = split_xy(scaler.transform(vali))\n",
        "        X_test_sc, y_test_sc = split_xy(scaler.transform(test))\n",
        "\n",
        "        # Fit, save best model and Predict\n",
        "        model.load_weights(\"empty_model.h5\", skip_mismatch=True, by_name=True)\n",
        "        model.reset_states()\n",
        "        history = model.fit(\n",
        "            X_train_sc, y_train_sc,\n",
        "            validation_data=(X_vali_sc, y_vali_sc),\n",
        "            epochs=200, # maximum number of epochs\n",
        "            batch_size=64, # better for jumping local minimas\n",
        "            verbose=False,\n",
        "            callbacks=[combined_cb, earlyStop, model_checkpoint]\n",
        "        )\n",
        "        model.load_weights(\"tmp_best_model.h5\", skip_mismatch=True, by_name=True)\n",
        "        preds_sc = model.predict(X_test_sc, verbose=False)\n",
        "\n",
        "        # Get Non-Scaled Data\n",
        "        X_train, y_train = split_xy(train.to_numpy())\n",
        "        X_vali, y_vali = split_xy(vali.to_numpy())\n",
        "        X_test, y_test = split_xy(test.to_numpy())\n",
        "        X_result, y_result = split_xy(scaler.inverse_transform(np.hstack((X_test_sc, preds_sc))))\n",
        "        ys = [y_train, y_vali, y_test, y_result]\n",
        "\n",
        "        # Compute Metrics\n",
        "        rmse = mean_squared_error(y_true=y_test, y_pred=y_result, squared=False)\n",
        "        mae = mean_absolute_error(y_true=y_test, y_pred=y_result)\n",
        "        mape = mean_absolute_percentage_error(y_true=y_test, y_pred=y_result)\n",
        "        r2 = r2_score(y_true=y_test, y_pred=y_result)\n",
        "        acc, cm = compute_accuracy_and_cm(y_vali, y_test, y_result)\n",
        "\n",
        "        metrics = [rmse, mae, mape, r2, acc]\n",
        "\n",
        "        # Plot All Curves and Metrics; Also loss curves\n",
        "        if flag_plot == 2:\n",
        "            plot_loss_curve(history, model_idx, i)\n",
        "            plot_predictions(dates, ys, metrics, model_idx, i)\n",
        "            plot_confusion_matrix(cm, metrics, model_idx, i)\n",
        "\n",
        "        # Plot last 5 Curves and Metrics; and bad fits, also the respective loss curves\n",
        "        elif (flag_plot == 1 and ((i >= (windows_cnt - 5)) or (r2 < 0) )):\n",
        "            plot_loss_curve(history, model_idx, i)\n",
        "            plot_predictions(dates, ys, metrics, model_idx, i)\n",
        "            plot_confusion_matrix(cm, metrics, model_idx, i)\n",
        "\n",
        "        # Append Result\n",
        "        cv_score.loc[len(cv_score)] = metrics\n",
        "\n",
        "    return cv_score\n",
        "\n",
        "\n",
        "# For each model, perform a cross validation training,\n",
        "# plot graphs and compute metrics if wanted\n",
        "cv_scores = []\n",
        "for i, model in enumerate(models):\n",
        "    model.compile(\n",
        "        loss=\"mean_squared_error\",\n",
        "        optimizer=Adam(learning_rate=0.0001)\n",
        "    )\n",
        "    model.save_weights(\"empty_model.h5\")\n",
        "    generator = sliding_window_generator(windowed_df)\n",
        "    cv_score = cross_validation(model, generator, i, 1)\n",
        "    cv_scores.append(cv_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmp65k36Yhf"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpYwJDOPGfRl"
      },
      "outputs": [],
      "source": [
        "# Output summary (mean, std, min, max)\n",
        "for i, cv_score in enumerate(cv_scores):\n",
        "    print(f\"Model {i}\")\n",
        "    print(cv_score.describe(), \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQlPTydvP8kk"
      },
      "outputs": [],
      "source": [
        "# Section by Parameter\n",
        "metrics = [\"RMSE\", \"MAE\", \"MAPE\", \"R2\", \"Acc\"]\n",
        "\n",
        "evaluation = {\n",
        "    \"RMSE\": {},\n",
        "    \"MAE\": {},\n",
        "    \"MAPE\": {},\n",
        "    \"R2\": {},\n",
        "    \"Acc\": {},\n",
        "}\n",
        "\n",
        "for model_idx, cv_score in enumerate(cv_scores):\n",
        "    for param in metrics:\n",
        "        evaluation[param][f\"Model {model_idx}\"] = cv_score[param].mean()\n",
        "\n",
        "for param in metrics:\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(param)\n",
        "    plt.bar(list(evaluation[param].keys()), list(evaluation[param].values()))\n",
        "    plt.xlabel(\"Models\")\n",
        "    plt.ylabel(\"Metrics Value\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(param + \" in Log Scale\")\n",
        "    plt.bar(list(evaluation[param].keys()), list(evaluation[param].values()), color=\"tab:orange\")\n",
        "    plt.xlabel(\"Models\")\n",
        "    plt.ylabel(\"Metrics Value\")\n",
        "    plt.yscale('log')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_xQ8UiKP8kl"
      },
      "outputs": [],
      "source": [
        "# Output complete results\n",
        "for i, cv_score in enumerate(cv_scores):\n",
        "    print(f\"Model {i}\")\n",
        "    print(cv_score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}